{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data_Cleaning_For_Sentiment_Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyBuie/DatasetCustomerSupportTweets/blob/main/Data_Cleaning_For_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmpb3rVdRyGq"
      },
      "source": [
        "### Known Issues\n",
        "### ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "### 1. Translator API only allows for 50 requests per day, so we can translate only 50 sentences. You are good to go if you have a paid version\n",
        "### 2. Emoji.demojize takes a long time to run on full dataset, I could only run it on 1 million records\n",
        "### ----------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diy26GffRyGs"
      },
      "source": [
        "### Create Anaconda environment and install the below packages using conda/pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVqKGMvhRyGu"
      },
      "source": [
        "#pip install emoji\n",
        "#pip install ftfy\n",
        "#pip install nltk\n",
        "#pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT1ZoPPje7rN"
      },
      "source": [
        "import os\n",
        "\n",
        "# Python utilities\n",
        "!import ftfy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import parser\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "!import emoji\n",
        "\n",
        "### Text processing\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import en_core_web_sm\n",
        "from google_trans_new import google_translator\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws7f1jIXRyGw"
      },
      "source": [
        "### Handy Functions\n",
        "\n",
        "### Ensure that the text encoding is utf\n",
        "def encode_data(text):\n",
        "    text = ftfy.fix_encoding(text)\n",
        "    return text\n",
        "### Convert text to lowercase\n",
        "def to_lower(text):\n",
        "    return text.lower()\n",
        "### Add items to stopwords\n",
        "def add_to_stopwords(set_stopwords):\n",
        "    for w in set_stopwords:\n",
        "        nlp.vocab[w].is_stop = True\n",
        "\n",
        "# Translator\n",
        "def translate(text):\n",
        "    time.sleep(2)\n",
        "    text_split = text.split()\n",
        "    if translator.detect(text_split)[0]!='en':\n",
        "        text = translator.translate(text)\n",
        "    else:\n",
        "        text = text\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw5gnyvHRyGw"
      },
      "source": [
        "### Variables & Data structures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5jXtm97RyGw"
      },
      "source": [
        "notebook_location = \"C:\\\\Users\\\\yamin\\\\Desktop\\\\Fiverr\\\\\" ## Add the location of the ipynb file\n",
        "filename = \"\\\\customer_support.csv\" # provide input file name here, I used a subset of data\n",
        "os.chdir(notebook_location) #changes your current working directory to where your notebook is\n",
        "path = os.getcwd() + filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW-_pkIXRyGx"
      },
      "source": [
        "translator = google_translator()\n",
        "nlp = en_core_web_sm.load()\n",
        "date_ = \"_\" + str(datetime.now()).split()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td65KLnPRKUA"
      },
      "source": [
        "### Used for contractions expansion , Abbreviation expansion ---- You can add your own\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
        "    \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
        "    \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
        " \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        " \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        " \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        " \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        " \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        " \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\",\n",
        " \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
        " \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
        " \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
        " \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        " \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        " \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        " \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        " \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        " \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
        "\"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\"i'm\":\"i am\",\"i m\\s\":\"i am \",\"im\":\"i am\",\"i've\":\"i have\",\"i'd\":\"i would\",\"i'll\":\"i will\",\"can't\":\"cannot\",\"couldn't\":\"could not\",\n",
        "\"that'll\":\"that will\",\"how's\":\"how is\",\"hasn't\":\"has not\",\"haven't\":\"have not\",\"e-mail\":\"email\",\"e mail\":\"email\",\"dm\":\"direct message\",\"dms\":\"direct message\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ME_W8hRyGz"
      },
      "source": [
        "### Regex Cleaner Class contains all possible combinations of characters that need to be removed from the text. You can add or remove functions to/from this. It also contains the functions that need to be run to clean the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuylFwMOe7rU"
      },
      "source": [
        "class TextRegexCleaner:\n",
        "    \n",
        "    \n",
        "    regex_data_cleaner = [\n",
        "    'https?://\\S+|www\\.\\S+', # Links\n",
        "    '\\S+@\\S+',\n",
        "    'From:(.*)\\r\\n',\n",
        "    'Subject:',\n",
        "    'Sent:(.*)\\r\\n',\n",
        "    'Received:(.*)\\r\\n',\n",
        "    'Received From:(.*)\\r\\n',\n",
        "    'To:(.*)\\r\\n',\n",
        "    'CC:(.*)\\r\\n',\n",
        "    'IC:(.*)\\r\\n',\n",
        "    'BCC:(.*)\\r\\n',\n",
        "    'IiNnCc[0-9]*',\n",
        "    'ticket[_]*[\\\\s]*[0-9]*',\n",
        "    '\\\\[cid:(.*)]',\n",
        "    '[0-9][\\\\-0â€“90-9 ]+',# phones\n",
        "    '[0-9]',# numbers\n",
        "    '[^a-zA-z 0-9]+',# anything that is not a letter\n",
        "    '[\\r\\n]',# single letters\n",
        "    ' [a-zA-Z] ',  # two-letter words\n",
        "    '  ', # double spaces\n",
        "    '\\&\\w*', # HTML special entities (e.g. &amp;)\n",
        "    '^[_a-z0-9-]+(\\\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\\\.[a-z0-9-]+)*(\\\\.[a-z]{2,4})$',\n",
        "    '[\\\\w\\\\d\\\\-\\\\_\\\\.]+ @ [\\\\w\\\\d\\\\-\\\\_\\\\.]+',\n",
        "    '[^a-zA-Z]',\n",
        "    \"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", #IP Address\n",
        "    \"\\s+\", # to remove extra whitespaces\n",
        "    ]\n",
        "    \n",
        " \n",
        "\n",
        "  ## Convert to Lower\n",
        "    def __lower(self,text):\n",
        "        return text.lower()\n",
        "    \n",
        "    def __is_date(self, word):\n",
        "        try:\n",
        "            parser.parse(word)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "  ## Unecessary unicode characters that are not required and might be gibberish  \n",
        "    def __remove_unwanted_unicodes(self, text):\n",
        "        processed = ' '.join([w for w in text.split() if not self.__is_date(w)])\n",
        "        processed= ''.join(c for c in processed if c <= '\\uFFFF')\n",
        "        return processed\n",
        "\n",
        "\n",
        "    def __remove_unreadable_characters(self, text):\n",
        "        unreadable_character_regex = \"[^\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\"\n",
        "        processed = ' '.join(re.sub(unreadable_character_regex, \" \", text).split())\n",
        "        return processed.strip()\n",
        "  \n",
        "    ## Expand contractions like I'm to I am, using the contractions dictionary\n",
        "    def __expand_contractions(self,text,key):\n",
        "        text = text.replace(key,contraction_dict[key])\n",
        "        return text  \n",
        "    \n",
        "    ## Remove all characters as defined in the regex_data_cleaner\n",
        "    def __remove_text(self, text, regex):\n",
        "        rgx = re.compile(regex)\n",
        "        if re.search(rgx,text):\n",
        "            text = rgx.sub(' ',text)\n",
        "        else:\n",
        "            text= text\n",
        "        return text\n",
        "    \n",
        "    ## Remove all digits\n",
        "    def __remove_digits(self, text):\n",
        "        return ''.join(i for i in text if not i.isdigit())\n",
        "   \n",
        "    ## Removes only single character(e.g. hi I am a good s girl; removes s)\n",
        "    def __remove_single_character_word(self, text):\n",
        "        return ' '.join(i for i in text.split() if len(i) > 0)\n",
        "    \n",
        "    ## Removes unecessary spaces\n",
        "    def __remove_spaces(self,text):\n",
        "        return text.strip()\n",
        "  \n",
        "    ## Orchestrator that calls all the above methods  \n",
        "    def process(self, text=None):\n",
        "        if text is None:\n",
        "            return text\n",
        "    \n",
        "        processed = text\n",
        "        for key in contraction_dict.keys():\n",
        "            processed = self.__expand_contractions(processed,key)\n",
        "        for regex in self.regex_data_cleaner:\n",
        "            processed = self.__remove_text(processed, regex)\n",
        "        processed = self.__remove_unwanted_unicodes(processed)\n",
        "        processed = self.__remove_unreadable_characters(processed)\n",
        "        processed = self.__remove_digits(processed)\n",
        "        processed = self.__remove_spaces(processed)\n",
        "\n",
        "        return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8ZKXn5sRyG1"
      },
      "source": [
        "### The below class removes stopwords(common words like the,a,an,any etc from the text and converts the word to its base form)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaeD40NQRyG1"
      },
      "source": [
        "class Stopword_Lemmatizer:\n",
        "    \n",
        "    \n",
        "    \n",
        "    def __lemmatize(self,text,lemmatizer):\n",
        "        tokens = re.split('\\W+',text)\n",
        "        text = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return ' '.join(text)\n",
        "    \n",
        "\n",
        "    def __remove_stopwords(self,text,stopwords):\n",
        "        tokens = re.split('\\W+',text)\n",
        "        text = [word for word in tokens if word not in stopwords and word]\n",
        "        return ' '.join(text)\n",
        "    \n",
        "    def process(self,text,stopwords,lemmatizer):\n",
        "        if text is None:\n",
        "            return text\n",
        "        processed = text\n",
        "        processed = self.__remove_stopwords(processed,stopwords)\n",
        "        processed = self.__lemmatize(processed,lemmatizer)\n",
        "        return processed\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwwq6uBxRyG2"
      },
      "source": [
        "### 1. Read the input csv & print its shape & check its columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1I8-dj0e7rW"
      },
      "source": [
        "df = pd.read_csv(path)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOSKAcI3RyG3"
      },
      "source": [
        "df = df[0:200000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LLj0lvZRyG3"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK9pIVSfRyG3"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29MUALhtRyG4"
      },
      "source": [
        "now = time.time()\n",
        "df['text'] = df['text'].apply(encode_data)\n",
        "print('minutes',(time.time() - now)/60)  # on 200,000 records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT_aOupkRyG4"
      },
      "source": [
        "### 2. The tweets are in sequence and can be changed to a conversation or these can be handled as individual tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWRFfTLFRyG4"
      },
      "source": [
        "### returns userid that is present at the beginning of a tweet\n",
        "def extract_user(text):\n",
        "    ptn = re.compile('[^a-zA-Z0-9@]')\n",
        "    otr = re.compile('[\\r\\n]')\n",
        "    if text[0] == '@':\n",
        "        text = text.split()[0]\n",
        "        text = ptn.sub('',text)\n",
        "        #text = otr.sub('',text)\n",
        "    else:\n",
        "        text = 'XXXXX'\n",
        "    return text.lower()\n",
        "\n",
        "#### these userids needs to be removed from the main text\n",
        "def remove_user(text):\n",
        "    if text[0] == '@':\n",
        "        text=text.split()\n",
        "        text=text[1:]\n",
        "        text = ' '.join(text)\n",
        "    return text.lower()\n",
        "\n",
        "## utilities for creating the conversation key\n",
        "\n",
        "dictionary={}\n",
        "def convert_dict(d):\n",
        "    for item in d:\n",
        "        dictionary[item] = ''\n",
        "    return dictionary\n",
        "def set_conversion(text):\n",
        "    txt = []\n",
        "    text = text.split('@')\n",
        "    for t in text:\n",
        "        if (t.strip()) and (str(t) != 'nan'):\n",
        "            txt.append(t)\n",
        "    txt = set(txt)\n",
        "    return txt\n",
        "\n",
        "dict_ ={}\n",
        "def create_key(col1, col2):\n",
        "    col1 = tuple(col1)\n",
        "    dict_[col1] = col1[0] + '_' +str(col2)\n",
        "    return dict_\n",
        "def replace_key(text):\n",
        "    text = tuple(text)\n",
        "    return key_dict[0][text]\n",
        "\n",
        "def split_sort(text):\n",
        "    text = text.split(',')\n",
        "    text = [float(i) for i in text if str(i) != 'nan']\n",
        "    return sorted(text,reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzy4BHjmRyG5"
      },
      "source": [
        "### 3. Remove UserIDs from the main text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DayYXUkVRyG5"
      },
      "source": [
        "df['text_stripped'] = df['text'].apply(remove_user)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exsF_7i9RyG5"
      },
      "source": [
        "### 4. Extracting combination of author_id & user_id  for each tweet so that a conversation key can be created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vcGA9t5RyG5"
      },
      "source": [
        "df['sent_to_user'] = df['text'].apply(extract_user)\n",
        "df['author_id'] = df['author_id'].astype(str)\n",
        "df['author_id'] = df['author_id'].apply(to_lower)\n",
        "df['from_user_to'] = df['author_id']+ df['sent_to_user']\n",
        "df['sent_to_user_concatenated'] = df['sent_to_user'] + '@' + df['author_id']\n",
        "df['involved_user_ids'] = df['sent_to_user_concatenated'] + '@' + df['from_user_to']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7xhvwbnRyG6"
      },
      "source": [
        "### 5. Using the extracted author_id & user_id from above to add to the stopwords list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9AJy0Y1RyG6"
      },
      "source": [
        "user_set = set(df['sent_to_user'])\n",
        "author_set = set(df['author_id'])\n",
        "stopwords_user_set = user_set | author_set\n",
        "add_to_stopwords(stopwords_user_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdMCYFI2RyG7"
      },
      "source": [
        "### 6. Creating a conversation key, that will bind together multiple tweets that belong to the same conversation using the author_id & user_id extracted above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9kaNZHRyG7"
      },
      "source": [
        "df['from_user_to'] = df['sent_to_user_concatenated'].apply(set_conversion)\n",
        "key_dict = df.apply(lambda x: create_key(x['from_user_to'], x['tweet_id']), axis=1)\n",
        "df['conversation_key'] = df['from_user_to'].apply(replace_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9eFUyQmRyG7"
      },
      "source": [
        "### Printing the conversation keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLu2RKURyG7"
      },
      "source": [
        "df['conversation_key'].value_counts() ## tweets having the same conversation_key are part of the same conversation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkgenkTZRyG8"
      },
      "source": [
        "### 7. Translating emojis to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTG8cFKaRyG8"
      },
      "source": [
        "now = time.time()\n",
        "df['text_stripped'] = df['text_stripped'].apply(lambda x: emoji.demojize(x)) \n",
        "print(\"minutes\",(time.time() - now)/60)  ## for 200,000 records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327TKrARRyG8"
      },
      "source": [
        "### 8. Translating to english - Some tweets are in a different language, so we can use the translator API to translate. But this API has a limit on the number of requests that we make. It's 50 currently... \n",
        "\n",
        "#### 1. Manually translate other language tweets in the csv using Google Translate Web UI\n",
        "#### 2. Manually assess how many other language tweets are there and then take only those tweets and batchwise translate from multiple user accounts using translator API. 50 per batch\n",
        "#### 3. Remove all other language tweets (Not recommended)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piDhgrcdRyG8"
      },
      "source": [
        "### This is a crude way to separate English and non English tweets using patterns - you may use it if not able to use the translator\n",
        "\n",
        "english_pattern = re.compile('[a-zA-Z0-9@,.!-/\\~#*&^%\\'?]')\n",
        "def detect_language(text,ptn):\n",
        "    if ptn.search(text):\n",
        "        text = ptn.sub('',text)\n",
        "        if text.strip():\n",
        "            return 'ne'\n",
        "        else:\n",
        "            return 'en'\n",
        "    else:\n",
        "        return 'ne'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpDkj5eFRyG9"
      },
      "source": [
        "now = time.time()\n",
        "df['language'] = df['text_stripped'].apply(detect_language,args=(english_pattern,))\n",
        "print(\"minutes\",(time.time() - now)/60) ## for 200,000 records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plXIi1i3RyG9"
      },
      "source": [
        "### Translation examples\n",
        "\n",
        "print(df.iloc[234]['text_stripped'])\n",
        "print(\"-\"*125)\n",
        "print(translate(df.iloc[234]['text_stripped']))\n",
        "print(\"-\"*125)\n",
        "print(df.iloc[1195]['text_stripped'])\n",
        "print(\"-\"*125)\n",
        "print(translate(df.iloc[1195]['text_stripped']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGSSjYyZRyG9"
      },
      "source": [
        "### 9. Grouping together tweets with the same conversation_key. Going to filter only the english records as I am unable to translate, you can ignore the first step(filtering by language == 'en') if using the full dataset and are able to translate all the non english tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7KDHLHlRyG-"
      },
      "source": [
        "df = df[df['language'] == 'en'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD_SPdTbRyG-"
      },
      "source": [
        "print(df.shape) ### out of 200,000 ;114,392 were identified as English"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdRNq7UtRyG_"
      },
      "source": [
        "now = time.time()\n",
        "df['conversation'] = df.groupby('conversation_key')['text_stripped'].transform(lambda x: ' '.join(x))\n",
        "print(\"minutes\",(time.time() - now)/60)  ## for 200,000 records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSsHo10SRyG_"
      },
      "source": [
        "df['conversation'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koZoyXpzRyG_"
      },
      "source": [
        "df = df.drop_duplicates(subset='conversation',keep='first')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbObVOdyRyG_"
      },
      "source": [
        "print(df.shape)  ## size reduced to 58,936 from 114,392"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hf8ZwBKRyG_"
      },
      "source": [
        "### 10. Removing Special Characters,numbers,links,emails,extra whitespaces,trailing and leading spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGQV07bBe7rZ"
      },
      "source": [
        "df_process = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBNEwr7ne7rZ"
      },
      "source": [
        "clean_text = TextRegexCleaner()\n",
        "now = time.time()\n",
        "df_process['cleaned_text'] = df_process['conversation'].apply(clean_text.process)\n",
        "print('minutes',(time.time() - now)/60) ### on 58,936 records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fldeOyTMe7rZ"
      },
      "source": [
        "df_process.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvFYBEbhe7ra"
      },
      "source": [
        "print(df_process.loc[99]['cleaned_text'])\n",
        "print(df_process.loc[99]['text_stripped'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5jFq6p-e7ra"
      },
      "source": [
        "## Stopwords removal and lemmatization, \n",
        "\n",
        "### 1. Words like 'a','the','any','an' etc are very frequent and common and so should be removed from our corpus, ### 2.  using and used and use can all be converted to their base form \"use\" , called lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5D52m6WRyHB"
      },
      "source": [
        "stopword_lemmatizer = Stopword_Lemmatizer()\n",
        "stopwords = set(STOP_WORDS)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8mJaZeOe7ra"
      },
      "source": [
        "now = time.time()\n",
        "df_process['feature_text'] = df_process['cleaned_text'].apply(stopword_lemmatizer.process,args=(stopwords,lemmatizer,))\n",
        "print(\"minutes\",(time.time() - now)/60) # on  58,936 records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKM8a4HIRyHB"
      },
      "source": [
        "model_subset = ['tweet_id','conversation_key','feature_text']\n",
        "df = df_process[model_subset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diYajinyRyHC"
      },
      "source": [
        "df.to_pickle(\"processed.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvMiWQUNRyHC"
      },
      "source": [
        "### End of Data Cleaning & Preprocessing\n",
        "\n",
        "### Suggestions\n",
        "\n",
        "### 1. Add more stopwords (remove Nouns using Parts Of Speech tagging,Named entity Recognition)\n",
        "### 2. Try a different approach to creating the conversation key, using the tweet_id,response_tweet_id and in_response_to_tweet_id\n",
        "### 3. Identify misspelled words and correct them or remove the misspelled words"
      ]
    }
  ]
}